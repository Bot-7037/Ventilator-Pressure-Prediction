Types of NN - ANN, CNN, RNN
We have used a RNN which in contrast to a feed forward network, retains data and hence has the ability to remember
A normal NN keeps a matrix of weights that is uses to make predictions, but a RNN tweaks the weight of both current and previous inputs
Types of RNN - 1->1, 1->many, many->1, many->many
We have used a many to one basically because we need to predict a single value, given a set of values
Forward propagation gives a output and if the output is wrong, we backpropagate to adjust the weights by performing : weights - (partial derivative of weeight wrt to weights)
Instead of performing simple backpropagation it performs BPTT(Back propagation through time)
BPTT means, instead of going through a layers normally, it "unrolls" the RNN and "goes back in time" to analyze the network
Gradient - Partial derivative wrt inputs. This shows how much an output changes wrt change in input
Problems with RNN - Exploding(Assigning a high importance to a weight without much reason) and Vanishing(When the gradients become too small and NN learns too slow) gradients
LSTM - LSTM come into play in order to solve the vanishing gradient problem
[How lstm solve vanishing gradient problems here]
LSTM assigns weights to data also and over time it comes to know which data is important and which is not
LSTM solve the issue of vanishing gradient by keeping the gradients steep enought throughout the training, hence keeping the learning rate high


